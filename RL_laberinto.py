'''
Este proyecto tiene como objetivo desarrollar un entorno de laberinto simple y aplicar un algoritmo de **Aprendizaje por Refuerzo** para ense침ar a una IA a navegar desde un punto inicial hasta un objetivo.

Dada la naturaleza de este proyecto, considero que el algoritmo m치s adecuado para este tipo de probleas es **Q-Learning**, por su facilidad de implelentaci칩n y comprensi칩n, su estabilidad y su relaci칩n entre la exploraci칩n y la explotaci칩n.

Por esa raz칩n te propongo resolverlo usando ese algoritmo, aunque dejo a tu criterio si quieres resolverlo con otro algoritmo de tu elecci칩n. Siempre estar칠 a favor de que investigues, y expandas las habilidades propuestas por tu cuenta.

### Descripci칩n del Laberinto:

El laberinto se representa como una matriz de dos dimensiones, donde cada elemento puede ser:
+ un camino libre (0)
+ un obst치culo (1)
+ el objetivo (G)

La tarea es desarrollar un agente que pueda aprender a encontrar el camino desde un punto de inicio hasta el objetivo evitando obst치culos.

### Creaci칩n del Laberinto

Debido a que el desaf칤o de hoy es bastante complejo, y que el objetivo final no se trata de que sepas desarrollar laberintos, sino sistemas para resolverlos, voy a facilitar la tarea entregando en este cuaderno el c칩digo para generar nuestros laberintos.

Tu parte ser치 la siguiente, que es dise침ar y entrenar un modelo de Q-Learning para resolver el laberinto de la manera m치s eficiente, y luego mostrar una visualizaci칩n sobre c칩mo lo ha hecho.

Te deseo toda la suerte del mundo, y sobre todo, que te diviertas de a montones.

'''

# Importar las librer칤as necesariass
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.widgets import Button, Slider
import random
import time

class LaberintoQLearning:
    def __init__(self):
        # Configuraci칩n inicial
        self.dimensiones = (10, 10)  # Tama침o por defecto
        self.porcentaje_obstaculos = 0.2  # 20% de obst치culos
        self.estado_inicial = None
        self.estado_objetivo = None
        self.Q = None
        self.obstaculos = []
        self.acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Arriba, abajo, izquierda, derecha
        self.entrenamiento_completado = False
        self.camino = []
        
        # Configuraci칩n de la interfaz gr치fica
        self.fig = plt.figure(figsize=(10, 10))
        self.fig.canvas.manager.set_window_title('Laberinto con Q-Learning')  # Corregido aqu칤
        plt.subplots_adjust(bottom=0.3)
        self.ax = self.fig.add_subplot(111)
        
        # Crear controles
        self.crear_controles()
        
        # Generar laberinto inicial
        self.generar_laberinto()
        
        # Mostrar instrucciones iniciales
        self.mostrar_mensaje("Selecciona la posici칩n inicial y luego la meta")
        
        # Conectar eventos
        self.fig.canvas.mpl_connect('button_press_event', self.on_click)
        
        plt.show()
    
    def crear_controles(self):
        # Bot칩n para reiniciar
        ax_reiniciar = plt.axes([0.3, 0.11, 0.4, 0.05])
        self.btn_reiniciar = Button(ax_reiniciar, 'Reiniciar Juego')
        self.btn_reiniciar.on_clicked(self.reiniciar_juego)
        
        # Bot칩n para comenzar entrenamiento
        ax_start = plt.axes([0.3, 0.03, 0.4, 0.05])
        self.btn_start = Button(ax_start, 'Comenzar Entrenamiento')
        self.btn_start.on_clicked(self.comenzar_entrenamiento)
        
        # Slider para tama침o del laberinto
        ax_tamano = plt.axes([0.2, 0.22, 0.6, 0.03])
        self.slider_tamano = Slider(
            ax=ax_tamano,
            label='Tama침o del Laberinto',
            valmin=5,
            valmax=20,
            valinit=10,
            valstep=1
        )
        self.slider_tamano.on_changed(self.actualizar_tamano)
        
        # Slider para densidad de obst치culos
        ax_obstaculos = plt.axes([0.2, 0.18, 0.6, 0.03])
        self.slider_obstaculos = Slider(
            ax=ax_obstaculos,
            label='Densidad de Obst치culos',
            valmin=0.1,
            valmax=0.4,
            valinit=0.2,
            valstep=0.05
        )
        self.slider_obstaculos.on_changed(self.actualizar_obstaculos)
    
    def generar_laberinto(self):
        """Genera un nuevo laberinto con obst치culos aleatorios y bordes bloqueados"""
        filas, columnas = self.dimensiones
        
        # Limpiar obst치culos anteriores
        self.obstaculos = []
        
        # Agregar bordes como obst치culos
        for i in range(filas):
            self.obstaculos.append((i, 0))  # Borde izquierdo
            self.obstaculos.append((i, columnas-1))  # Borde derecho
            
        for j in range(columnas):
            self.obstaculos.append((0, j))  # Borde superior
            self.obstaculos.append((filas-1, j))  # Borde inferior
        
        # Calcular n칰mero de obst치culos adicionales (excluyendo bordes)
        celdas_totales = filas * columnas
        celdas_borde = 2 * (filas + columnas) - 4
        celdas_interiores = celdas_totales - celdas_borde
        num_obstaculos = int(celdas_interiores * self.porcentaje_obstaculos)
        
        # Generar obst치culos aleatorios en el interior
        celdas_interiores = [(i, j) for i in range(1, filas-1) for j in range(1, columnas-1)]
        self.obstaculos.extend(random.sample(celdas_interiores, num_obstaculos))
        
        # Reiniciar estados y Q-table
        self.estado_inicial = None
        self.estado_objetivo = None
        self.Q = np.zeros((filas * columnas, len(self.acciones)))
        self.entrenamiento_completado = False
        self.camino = []
        
        # Redibujar
        self.dibujar_laberinto()
    
    def dibujar_laberinto(self):
        """Dibuja el laberinto en el gr치fico"""
        self.ax.clear()
        filas, columnas = self.dimensiones
        
        # Configurar ejes
        self.ax.set_xlim(-0.5, columnas-0.5)
        self.ax.set_ylim(-0.5, filas-0.5)
        self.ax.set_xticks(np.arange(-0.5, columnas, 1), minor=True)
        self.ax.set_yticks(np.arange(-0.5, filas, 1), minor=True)
        self.ax.grid(which="minor", color='black', linestyle='-', linewidth=2)
        self.ax.set_xticks([])
        self.ax.set_yticks([])
        
        # Dibujar obst치culos
        for obs in self.obstaculos:
            self.ax.add_patch(patches.Rectangle(
                (obs[1]-0.5, obs[0]-0.5), 1, 1,
                facecolor='#2c3e50', edgecolor='black'
            ))
        
        # Dibujar inicio y meta si est치n definidos
        if self.estado_inicial:
            self.ax.text(
                self.estado_inicial[1], self.estado_inicial[0],
                'I', ha='center', va='center', fontsize=20
            )
        
        if self.estado_objetivo:
            self.ax.text(
                self.estado_objetivo[1], self.estado_objetivo[0],
                'F', ha='center', va='center', fontsize=20
            )
        
        # Dibujar camino si est치 definido
        for paso in self.camino:
            self.ax.add_patch(patches.Circle(
                (paso[1], paso[0]), 0.3,
                color="#34c024", alpha=0.7
            ))
        
        plt.draw()
    
    def mostrar_mensaje(self, mensaje):
        """Muestra un mensaje en la parte superior del gr치fico"""
        self.ax.set_title(mensaje, fontsize=14, pad=20)
        plt.draw()
    
    def on_click(self, event):
        """Maneja los clics del usuario para seleccionar inicio y meta"""
        if event.inaxes != self.ax or self.entrenamiento_completado:
            return
        
        # Obtener posici칩n del clic
        col, fila = int(round(event.xdata)), int(round(event.ydata))
        pos = (fila, col)
        
        # Validar posici칩n
        if not self.es_valida(pos):
            self.mostrar_mensaje("춰Posici칩n inv치lida! Elige un espacio libre.")
            return
        
        # Seleccionar inicio o meta
        if self.estado_inicial is None:
            self.estado_inicial = pos
            self.mostrar_mensaje("Ahora selecciona la posici칩n de la meta (游끥)")
        elif self.estado_objetivo is None:
            self.estado_objetivo = pos
            self.mostrar_mensaje("춰Listo! Presiona 'Comenzar Entrenamiento'")
        else:
            self.mostrar_mensaje("Ya se seleccionaron inicio y meta")
        
        self.dibujar_laberinto()
    
    def es_valida(self, pos):
        """Verifica si una posici칩n es v치lida (dentro del laberinto y no es obst치culo)"""
        return (
            0 <= pos[0] < self.dimensiones[0] and
            0 <= pos[1] < self.dimensiones[1] and
            pos not in self.obstaculos
        )
    
    def comenzar_entrenamiento(self, event):
        """Inicia el proceso de entrenamiento con Q-Learning"""
        if self.estado_inicial is None or self.estado_objetivo is None:
            self.mostrar_mensaje("춰Selecciona inicio y meta primero!")
            return
        
        self.mostrar_mensaje("Entrenando al agente...")
        plt.pause(0.1)  # Permitir que se actualice la interfaz
        
        # Par치metros de Q-Learning
        alpha = 0.1  # Tasa de aprendizaje
        gamma = 0.9  # Factor de descuento
        epsilon = 0.2  # Probabilidad de exploraci칩n
        episodios = 500  # N칰mero de episodios de entrenamiento
        
        # Inicializar Q-table si es necesario
        if self.Q is None:
            self.Q = np.zeros((self.dimensiones[0] * self.dimensiones[1], len(self.acciones)))
        
        # Funci칩n para convertir estado a 칤ndice
        def estado_a_indice(estado):
            return estado[0] * self.dimensiones[1] + estado[1]
        
        # Entrenamiento
        for episodio in range(episodios):
            estado = self.estado_inicial
            terminado = False
            
            while not terminado:
                # Elegir acci칩n (풧-greedy)
                if random.random() < epsilon:
                    accion_idx = random.randint(0, len(self.acciones)-1)
                else:
                    accion_idx = np.argmax(self.Q[estado_a_indice(estado)])
                
                # Aplicar acci칩n
                accion = self.acciones[accion_idx]
                nuevo_estado = (estado[0] + accion[0], estado[1] + accion[1])
                
                # Calcular recompensa
                if not self.es_valida(nuevo_estado):
                    recompensa = -10
                    terminado = False
                    nuevo_estado = estado  # Permanece en el mismo estado
                elif nuevo_estado == self.estado_objetivo:
                    recompensa = 100
                    terminado = True
                else:
                    recompensa = -1
                    terminado = False
                
                # Actualizar Q-table
                self.Q[estado_a_indice(estado), accion_idx] += alpha * (
                    recompensa + 
                    gamma * np.max(self.Q[estado_a_indice(nuevo_estado)]) - 
                    self.Q[estado_a_indice(estado), accion_idx]
                )
                
                estado = nuevo_estado
            
            # Actualizar progreso cada 50 episodios
            if episodio % 50 == 0:
                self.mostrar_mensaje(f"Entrenando... Episodio {episodio}/{episodios}")
                plt.pause(0.01)
        
        # Entrenamiento completado
        self.entrenamiento_completado = True
        self.mostrar_mensaje("춰Entrenamiento completado! Mostrando camino...")
        
        # Mostrar el camino 칩ptimo
        self.mostrar_camino_optimo()
    
    def mostrar_camino_optimo(self):
        """Muestra el camino 칩ptimo aprendido por el agente"""
        if not self.entrenamiento_completado or self.estado_inicial is None or self.estado_objetivo is None:
            return
        
        # Reconstruir el camino
        estado = self.estado_inicial
        self.camino = [estado]
        
        def estado_a_indice(estado):
            return estado[0] * self.dimensiones[1] + estado[1]
        
        while estado != self.estado_objetivo:
            accion_idx = np.argmax(self.Q[estado_a_indice(estado)])
            estado = (estado[0] + self.acciones[accion_idx][0], 
                     estado[1] + self.acciones[accion_idx][1])
            self.camino.append(estado)
            
            # Actualizar visualizaci칩n paso a paso
            self.dibujar_laberinto()
            plt.pause(0.1)
        
        self.mostrar_mensaje("춰Camino 칩ptimo encontrado!")
    
    def reiniciar_juego(self, event):
        """Reinicia el juego con la configuraci칩n actual"""
        self.generar_laberinto()
        self.mostrar_mensaje("Selecciona la posici칩n inicial y luego la meta")
    
    def actualizar_tamano(self, val):
        """Actualiza el tama침o del laberinto"""
        nuevo_tamano = int(self.slider_tamano.val)
        if nuevo_tamano != self.dimensiones[0]:
            self.dimensiones = (nuevo_tamano, nuevo_tamano)
            self.generar_laberinto()
            self.mostrar_mensaje("Selecciona la posici칩n inicial y luego la meta")
    
    def actualizar_obstaculos(self, val):
        """Actualiza la densidad de obst치culos"""
        self.porcentaje_obstaculos = self.slider_obstaculos.val
        self.generar_laberinto()
        self.mostrar_mensaje("Selecciona la posici칩n inicial y luego la meta")

# Iniciar la aplicaci칩n
if __name__ == "__main__":
    app = LaberintoQLearning()